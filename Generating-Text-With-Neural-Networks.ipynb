{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e628dfa5",
   "metadata": {},
   "source": [
    "GUID: 2660237\n",
    "GitHub Link: [\"Github\"](https://github.com/Natasha-Warder/AI.Python.github.io.-)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cdf46",
   "metadata": {},
   "source": [
    "# Generating Text with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0508a118",
   "metadata": {},
   "source": [
    "As part of the code provided, the URL of the text file is downloaded and its contents are read into the variable shakespeare_text. You can use this for a variety of natural language processing tasks or text generation tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e520949c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80]) # By printing the first 80 characters of shakespeare_text, you can see how the text is structured and what it says. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9a9b4f",
   "metadata": {},
   "source": [
    "Inspecting a portion of the data is a good way to understand its structure and format. In addition to preprocessing data, it can be useful to get an idea of what the text looks like simply by looking at it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\", # specifies that the text should be tokenized into individual characters \n",
    "                                                   standardize=\"lower\") # specifies that the tokenized text should be in lowercase\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "040dc8fc",
   "metadata": {},
   "source": [
    "TensorFlow's TextVectorization layer appears to be used to convert text data into numerical format, specifically using character-level tokenization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94735fca",
   "metadata": {},
   "source": [
    "The text is tokenized using the TextVectorization layer.\n",
    "The vocabulary size is calculated, and the dataset is split into training, validation, and test sets using the to_dataset function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)  #creates a TensorFlow dataset from the input sequence.\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True) #ensures that only complete windows are kept, discarding any partial windows.\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720d3ea4",
   "metadata": {},
   "source": [
    "If the shuffle parameter is True, the dataset is shuffled. The buffer size for shuffling is set to 100,000, and a seed is provided for reproducibility.\n",
    "\n",
    "The function below is responsible for converting the sliced data into a TensorFlow dataset with the specified parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42) #if you run the code again with the same seed, you should get the same random results\n",
    "train_set = to_dataset(encoded[:100_000], length=length, shuffle=True, #creates the training dataset\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[100_000:160_000], length=length) #creates the validation dataset\n",
    "test_set = to_dataset(encoded[160_000:], length=length) #creates the test dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614f4c1e",
   "metadata": {},
   "source": [
    "length = 100: represents the preferred length of sequences datasets\n",
    "\n",
    "\n",
    "train_set =...\n",
    "\n",
    "It takes the first 100,000 elements of the encoded data and uses the to_dataset function to convert it into a TensorFlow dataset and shuffles data. \n",
    "\n",
    "valid_set =...\n",
    "\n",
    "It takes elements 100,000 to 160,000 of the encoded data and converts it to a TensorFlow dataset and does not shuffle data. \n",
    "\n",
    "test_set =... \n",
    "\n",
    "It takes elements from index 160,000 to the end of the encoded data and converts it to a TensorFlow dataset and does not shuffle data. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# Building and Training the Model\n",
    "Building and training a model typically involves defining the model architecture, compiling the model, and then fitting it to the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19e4cf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code â€“ ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16), # mapping the input tokens to vectors of size 16\n",
    "    tf.keras.layers.GRU(128, return_sequences=True), # GRU is A Gated Recurrent Unit\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\") # output layer with a number of units equal to the number of tokens in the vocabulary\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint( # saves the model with the best validation accuracy during trainin\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353114cd",
   "metadata": {},
   "source": [
    "The training process is monitored, and the best model based on validation accuracy is saved. After training, the history object contains information about the training and validation metrics over epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf447f3",
   "metadata": {},
   "source": [
    "This code extends the previously defined model by incorporating a text vectorization layer (text_vec_layer). A Lambda layer is also applied to subtract 2 from the input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# Generating Text\n",
    "\n",
    "To generate text using the trained model, the basic idea of \"text generation\"is to input a seed sequence of text into the trained model. This means sampling the next character based on the model's predictions and adjusting the level of randomness through the use of temperature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0581742",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to be\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2] #Converts the character ID back to the corresponding word using the text vectorization layer's vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature # predicts the probabilities of the next character given the input sequence \n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1): #\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code â€“ ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d74ef5c",
   "metadata": {},
   "source": [
    "# Adjusting Temperature & Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=0.01)) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7b988d",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The seed text is \"To be or not to be,\" which serves as the starting point for text generation.The extend_text function generates additional text based on the seed text by iteratively predicting the next word/token and appending it to the generated text.The temperature parameter is set to a low value of 0.01. In the context of text generation, temperature controls the randomness of the predictions:\n",
    "Lower temperature (close to 0) leads to more deterministic predictions.\n",
    "Higher temperature introduces more randomness into the predictions.\n",
    "\n",
    "### Output Analysis:\n",
    "With a low temperature, the generated text is expected to exhibit more determinism. This means that the predictions are more likely to follow a specific pattern, resulting in a more structured and less varied output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=1)) # Moderate temperature (around 1) balances randomness and predictability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a66bb6f",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The seed text is still \"To be or not to be,\" but the temperature parameter is set to 1, a higher temperature value. In text generation, the temperature controls the randomness of the generated text. A high temperature leads to more diverse and random predictions.\n",
    "The function uses a trained model to predict the next tokens in the sequence based on the seed text. \n",
    "\n",
    "### Output Analysis:\n",
    "With a high temperature, the function applies a larger factor to the predicted probabilities, making the distribution more uniform and introducing more randomness. This results in the model making less deterministic and more varied choices for the next tokens by incorporating the word 'be' inside of the word 'bear'. \n",
    "\n",
    "Therefore, although high temperature can result in more creative and diverse text but may sacrifice coherence and structure. The balance between temperature values allows you to control the trade-off between randomness and determinism in the generated text. \n",
    "\n",
    "Experimenting with different temperature settings can provide insights into the behavior of the model and help achieve the desired level of creativity or predictability in the generated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(extend_text(\"To be or not to be\", temperature=100)) # High temperature leads to more diverse and random predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6a15b7",
   "metadata": {},
   "source": [
    "## Findings\n",
    "The seed text is still \"To be or not to be,\" but the temperature parameter is set to 100, which is a significantly higher temperature value.  A drastically higher temperature leads to extreme random predictions.\n",
    "\n",
    "### Output Analysis:\n",
    "With a high temperature, the function applies a larger factor to the predicted probabilities, making the distribution more uniform and introducing more randomness. This results in the model incomprehensible information for human readers 'To be or not to bef ,mt'&t3fpady-$\n",
    "wh!nse?pws&ertj-vgerdq,!c-yje,znq' data which includes randomised punctuation and symbols. \n",
    "\n",
    "This emphaises high temperature sacrificing coherence and structure. The inbalance between temperature values loses control between randomness and determinism in the generated text. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468c17fd",
   "metadata": {},
   "source": [
    "# Reflection\n",
    "\n",
    "Firstly, building and training the model 'INVALID_ARGUMENT: You must feed a value for placeholder tensor '...' with dtype int32'occurs multiple times. The main theory to its delayed operation Model Saving/Loading Warning: untraced functions during model saving, functions in the model might not be traceable after loading. Equally errors could occur due to Input Shape Mismatch or Placeholder Name Mismatch. \n",
    "\n",
    "Secondly, generating text faced model architecture issues. Custom layers or operations may be defined correctly and placeholders may need explicit initialization. ultimately, I beleive its inability to function could be a product of input data mismatch. \n",
    "\n",
    "Input data provided to the TensorFlow model during training or inference matches the expected input shape and data type. This could be improved by inspecting the model's structure to identify the placeholder tensors mentioned in the error message: (gradients/split_2_grad/concat/split_2/split_dim, gradients/split_grad/concat/split/split_dim, gradients/split_1_grad/concat/split_1/split_dim).\n",
    "in order to verify that these tensors are properly defined in this model, and their names match what the model is expecting."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
