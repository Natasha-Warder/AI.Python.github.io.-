{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62a1b437",
   "metadata": {},
   "source": [
    "GUID: 2660237\n",
    "GitHub Link: [\"Github\"](https://github.com/Natasha-Warder/AI.Python.github.io.-)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b5028e",
   "metadata": {},
   "source": [
    "# Critically Engaging with AI Ethics\n",
    "\n",
    "## Task 1: Initial Ideas\n",
    "\n",
    "## \"In this lab we will be critically engaging with existing datasets that have been used to address ethics in AI.\"\n",
    "\n",
    "In particular, we will explore the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge). \n",
    "This challenge brought to light bias in the data that sparked the [Jigsaw Unintended Bias in Toxicity Classification Challenge](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification).\n",
    "\n",
    "\n",
    "The Jigsaw Toxic Comment Classification Challenge was a machine learning competition hosted on Kaggle. The competition aimed to develop models capable of classifying toxic comments in online discussions. Toxic comments are those that are offensive, inflammatory, or otherwise likely to harm the online conversation.\n",
    "\n",
    "Participants in the challenge were provided with a large dataset of comments labeled for toxicity and were tasked with creating machine learning models that could accurately predict whether a given comment was toxic or not. The competition was organized by Jigsaw, a technology incubator created by Google to address societal issues through technology.The challenge sought to promote the development of effective and fair content moderation systems to improve online conversation spaces by automatically identifying and filtering out toxic comments. Participants employed various natural language processing (NLP) and machine learning techniques to build models capable of discerning toxic from non-toxic comments. \n",
    "\n",
    "#### Lab aims: to explore the dataset biases, to explore fairness in AI relative to aspects like demography and equal opportunity, analyse performance and group unawareness of the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba3a8e5",
   "metadata": {},
   "source": [
    "## Task 2: Identifying Bias\n",
    "\n",
    "\n",
    "The [Kaggle tutorial on Identifying Bias in AI](https://www.kaggle.com/code/alexisbcook/identifying-bias-in-ai/tutorial) emphasises how biases can creep in at any stage of the AI task. \n",
    "\n",
    "This aritcle highlights the dual nature of machine learning (ML), emphasizing its potential to improve lives as well as its capacity to cause harm. It acknowledges instances where ML applications have demonstrated discrimination based on factors such as race, sex, religion, socioeconomic status, and other categories. The primary focus of the tutorial is on bias, defined as the negative and undesired outcomes of ML applications, particularly when these outcomes disproportionately impact specific groups.\n",
    "\n",
    "\n",
    "- In this section we will explore the [Jigsaw Toxic Comment Classification Challenge](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) to discover different types of biases that might emerge in the dataset. \n",
    "\n",
    "The competition task involves scoring approximately fourteen thousand comments based on pairwise comparisons made by expert raters. Participants are challenged to build a model that can replicate the rankings provided by these professional raters. The evaluation metric is determined by the participants' average agreement with the expert raters across several hundred thousand rankings.\n",
    "\n",
    "The advantages of this approach are highlighted, such as the reduced difficulty in choosing between two comments compared to individually assessing each comment in isolation. However, the statement acknowledges the potential random selection when both comments are non-toxic in pairwise comparisons. To mitigate this, the competition emphasizes the importance of inter-annotator agreement, stating that when one comment is clearly more toxic, agreement among human raters tends to be higher.\n",
    "\n",
    "Overall, the competition poses a challenging task of ranking comment toxicity, acknowledging the complexities of individual judgment while leveraging pairwise comparisons to enhance the robustness of the evaluation process. The goal is to develop models that align with human expert assessments and effectively rank the severity of comment toxicity across a spectrum from innocuous to outrageous.\n",
    "\n",
    "\n",
    "## Task 2a: Understanding the Scope of Bias\n",
    "\n",
    "- How many types of biases are described on the page? \n",
    "Six Biases are decribed: historical, representation, measurement, aggregation, deployment, and evaluation. \n",
    "\n",
    "- Which type of bias did you know about already before this course and which type was new to you? \n",
    "Mainly historical, representation and evaluation which are socially concerned - inequalites featured by underepresenting specific minorities in studies or historically established social constructs which also contribute to flawed data.\n",
    "\n",
    "Unfamiliar with:\n",
    "\"Aggregation bias occurs when groups are inappropriately combined, resulting in a model that does not perform well for any group or only performs well for the majority group.\"\n",
    "\n",
    "\"Deployment bias occurs when the problem the model is intended to solve is different from the way it is actually used.\" \n",
    "\n",
    "Directly related to how data is handled usually in control of whoever is generating the database/organising the study. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586379a5",
   "metadata": {},
   "source": [
    "## Alternative examples of bias\n",
    "\n",
    "### To research data bias with AI I prompted chat GBT, the first five examples aligned with six biases previously mentioned:\n",
    "\n",
    "\n",
    "AI systems can exhibit various types of bias, which may arise from the data used to train them, the algorithms employed, or the design choices made during their development. \n",
    "\n",
    "Here are some common types of bias in AI:\n",
    "\n",
    "\n",
    "\n",
    "Sampling Bias: If the training data is not representative of the real-world population, the AI system may produce biased results. For example, if a facial recognition system is trained primarily on data of a certain ethnicity, it may perform poorly for other ethnicities.\n",
    "\n",
    "Algorithm Design Bias: The algorithms themselves can be biased based on the choices made during their design. For example, if an algorithm relies on certain features that are correlated with protected attributes (such as race or gender), it may lead to discriminatory outcomes.\n",
    "\n",
    "Objective Function Bias: The optimization objective used during training can introduce bias. If the objective function doesn't align with fairness goals, the AI system may optimize for the wrong outcomes.\n",
    "Interaction Bias:\n",
    "\n",
    "User Interaction Bias: Bias can be introduced through user interactions and feedback. If the system is used more frequently by a certain demographic, it may become better tuned to their preferences, potentially marginalizing other user groups.\n",
    "Deployment Bias:\n",
    "\n",
    "Contextual Bias: The environment in which the AI system is deployed may introduce bias. For instance, a language translation model trained on formal texts may perform poorly when translating informal or colloquial language.\n",
    "Aggregation Bias:\n",
    "\n",
    "\n",
    "### However there were two mentioned that did not align with six biases previously mentioned:\n",
    "\n",
    "Labeling Bias: Biases in the labels assigned to the training data can influence the AI system. If the training data is labeled with existing human biases, the AI system may learn and perpetuate those biases.\n",
    "\n",
    "### Human bias specific to labelling is a key example of how human perceptions are further subconciously transferred to data and organisation. \n",
    "\n",
    "Ensemble Bias: When multiple models are combined into an ensemble, biases present in individual models can aggregate and amplify, leading to biased ensemble predictions.\n",
    "\n",
    "### Moreover, ensemble bias or collective models exemplify the amplification of data based off of accumulated quantity. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3adc810",
   "metadata": {},
   "source": [
    "## Task 2b: Kaggle tutorial page \n",
    "Run through of the tutorial and evaluating screenshots of completed activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420da62c",
   "metadata": {},
   "source": [
    "### SCREENSHOT 1\n",
    "Initially the tutorial asks users to change a positive statement to a negative statement to test the model indicating toxicity. When muliple comments were added the last comment became the resultant. Ultimately, the model performed as suspected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abf4459",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/tutorial1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541a7877",
   "metadata": {},
   "source": [
    "### SCREENSHOT 2 \n",
    "\n",
    "The inputs become more sophisticated specific to race and religion. The model expresses western notions having a christian or white friend as not toxic. Whereas having a 'black' or 'muslim' friend creates the toxic output.\n",
    "\n",
    "My initial perceptions of statements were not negative. Particularly as people that are white and christian tend to justify racism or islamaphobia with statements such as \"I have a black friend\" or \"I have a muslim friend\". Therefore, I initially perceived the toxic output as a recognition of such social conventions. Which the model did not intent, but instead suggested the statement itself was wholly negative. \n",
    "\n",
    "Similarly, for someone not originating of western background or religion \"I have a white friend' or \"I have a christian friend' implies although the speaker themselves is not those things they are accepting of those things. Yet again, I read this optimistically and assumed the non-toxic output was in favour of diversity. \n",
    "\n",
    "This activity goes to show bias existed within my readings of statements which contrasted the models output. \n",
    "\n",
    "If the model is evaluated based on comments from users in the United Kingdom to users in Australia, emphasising evaluation bias and deployment bias. \n",
    "\n",
    "The model will also have representation bias the model is built to serve users in Australia, but was trained with data from users based in the United Kingdom.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6777d386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/tutorial2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92da883",
   "metadata": {},
   "source": [
    "### SCREENSHOT 3\n",
    "The model assigns each of roughly 58,000 words a coefficient, where higher coefficients denote words that the model thinks are more toxic. The code cell outputs the ten words that are considered most toxic, along with their coefficients.\n",
    "\n",
    "When asked if there are any words that should not be in the list, the input no was correct - This served as an effective trick question which allows users to determine what they view as a toxic output. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ec2857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/tutorial3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b38fa2",
   "metadata": {},
   "source": [
    "# Task 3: Large Language Models and Bias: Word Embedding Demo\n",
    "\n",
    "Go to the [embedding projector at tensorflow.org](http://projector.tensorflow.org/). This may take some time to load so be patient! There is a lot of information being visualised. This will take especially long if you select \"Word2Vec All\" as your dataset. The projector provides a visualisation of the langauge language model called **Word2Vec**.\n",
    "\n",
    "exploring the relationships between the words in the **Word2Vec** model. \n",
    "First, select **Word2Vec 10K** from the drop down menu (top lefthand side).\n",
    "This is a reduced version of **Word2Vec All**. You can search for words by submitting them in the search box on the right hand side. \n",
    "\n",
    "## Task 3.1: Initial exploration of words and relationships\n",
    "\n",
    "Typing `apple` and clicking on `Isolate 101 ppints`reduces the noise. Juice, fruit, wine are closer together than macintosh, computers and atari. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29cda119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/apple.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4860d8c9",
   "metadata": {},
   "source": [
    "Words like `silver` and `sound`. Turned no results  words related to eachother ususally sit close to eachother. In this case results are zero and dispersed further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfcf8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/sound.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf7c2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/sound.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e09c75",
   "metadata": {},
   "source": [
    "## Task 3.2: Exploring \"Word2Vec All\" for patterns\n",
    "\n",
    "Searching the occupation 'army' the were a multitude of connections related to manual-labour\n",
    "himself, men, manpower,and the names of chieftons, kings, admirals. \n",
    "\n",
    "or terms related to the occupation which were gender-neutral: employee, hero. \n",
    "\n",
    "the term 'army' excluded anything fully feminine aside from gendered roles like grandmother (maternal), witch (villainised feminine). \n",
    "Therefore there are concerns for gender bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18968b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/army.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eec6da8",
   "metadata": {},
   "source": [
    " # attribution to the authors of the Projector demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffeb50",
   "metadata": {},
   "source": [
    "# Task 4: Thinking about AI Fairness \n",
    "\n",
    "The previous exercise about the machine learning pipeline and analysis of the embedding projector hihglights how the assessment of datasets can be crucicial to deciding the suitability of deploying AI in the real world. The [Kaggle Tutorial on AI Fairness](https://www.kaggle.com/code/alexisbcook/ai-fairness) is used to connect data to questions of fairness.\n",
    "\n",
    "## Task 4-a: Topics in AI Fairness\n",
    "\n",
    "- How many criteria are described on the page? \n",
    "4 criteria are described on the page: Demographic parity / statistical parity, equal  opportunity, equal accuracy, Group unaware / \"Fairness through unawareness\". \n",
    "\n",
    "Familiar criteria included: \n",
    "- Group unaware, fairness through unawareness can be used for focus groups to test products or create unbiased opinions for studies by being unaware of the conclusive topic/question. \n",
    "- Equal opportunity fairness ensures that the proportion of people who should be selected by the model (\"positives\") that are correctly selected by the model is the same for each group.\n",
    "None of them were unknown criteria. \n",
    "\n",
    "## Alternative Criteria \n",
    "### To research alternative fairness criteria, these four factors develop or contrast previous points:\n",
    "\n",
    "Procedural Fairness: Ensure fairness in the procedures and processes involved in the development and deployment of AI systems.\n",
    "\n",
    "User-Centered Development: Prioritize the needs and perspectives of end-users to create AI systems that align with human values and societal norms.\n",
    "Mitigating Unintended Consequences:\n",
    "\n",
    "Risk Assessment: Conduct thorough risk assessments to identify and mitigate potential negative consequences of AI systems.\n",
    "\n",
    "Iterative Development: Regularly monitor AI systems in real-world contexts and make continuous improvements to address emerging issues and challenges.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1264c12a",
   "metadata": {},
   "source": [
    "# Task 5: AI and Explainability\n",
    "\n",
    "The reduction in a model score that results from randomly shuffling a single feature value is known as the permutation feature importance . The model score dropping indicates how much the model depends on the feature because this operation breaks the connection between the feature and the target. Features are what you might consider the columns in a tabulated dataset, such as that might be found in a spreadsheet. The more your AI performance gets messed up in response to the shuffling, the more likely the feature was important for the AI model.\n",
    " \n",
    "READING: [Tutorial on Permutation Importance](https://www.kaggle.com/code/dansbecker/permutation-importance) at Kaggle. The page describes an example to \"predict a person's height when they become 20 years old, using data that is available at age 10\". \n",
    "\n",
    "KEY POINTS:\n",
    "- Permutation importance is calculated after a model has been fitted. \n",
    "- Process = 1. Get a trained model.2. Shuffle the values in a single column, make predictions using the resulting dataset. 3.Use these predictions and the true target values to calculate how much the loss function suffered from shuffling. 4. That performance deterioration measures the importance of the variable you just shuffled.\n",
    "- When interpreting permutation importances the values towards the top are the most important features, and those towards the bottom matter least.\n",
    "\n",
    "\n",
    "\n",
    "#### Task 1-a: Carry out the exercise, taking screenshots of the exercise as you make progress. Using screen shots and text in your notebook, answer the following question: \n",
    "1. How many features are in this dataset?\n",
    "pickup_longitude\n",
    "pickup_latitude\n",
    "dropoff_longitude\n",
    "dropoff_latitude\n",
    "passenger_count\n",
    " a total of 5 \n",
    "\n",
    "2. Were the results of doing the exercise contrary to intuition? \n",
    "No - initially amount of passengers as the outlier was unsuprising data and the feature importance for latitudinal distance is greater than the importance of longitudinal distance. \n",
    "\n",
    "#### Task 1-b: Reflecting on Permutation Importance.\n",
    "\n",
    "Permutation importance is a reasonable measure of feature importance: It can be applied to any machine learning model, regardless of its underlying algorithm, making it a versatile approach. It captures non-linear relationships between features and the target variable, as it perturbs the actual values of the features.\n",
    "An issue could be because it accounts for the interdependence between features, shuffling one feature may affect others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ca0b505",
   "metadata": {},
   "source": [
    "#### Ran the initial code to visualise data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d271dfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/taxi1.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81e5b66",
   "metadata": {},
   "source": [
    "#### removing comment established importance, excluding passenger count. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508bf644",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/taxi2.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07377a3b",
   "metadata": {},
   "source": [
    "#### The code below creates new features for longitudinal and latitudinal distance. It then builds a model that adds these new features to those you already had. Finally Latitude distances may typically be longer than longitude distances when travelling. Rearranging the longitude values wouldn't have as much of an impact if they were typically closer together. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439db0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image (\"images/taxi3.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44978d66",
   "metadata": {},
   "source": [
    "# Task 6: Further Activities for Broader Discussion\n",
    "\n",
    "Apart from the [**Jigsaw Toxic Comment Classification Challenge**](https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge) another challenge you might explore is the [**Inclusive Images Challenge**](https://www.kaggle.com/c/inclusive-images-challenge). Read at least one of the following.\n",
    "\n",
    "- Article summarising [the Inclusive Image Challenge at NeurIPS 2018 conference](https://link.springer.com/chapter/10.1007/978-3-030-29135-8_6)\n",
    "\n",
    "The competition aims to assess the capacity of trained image captioning systems to generalize effectively to images originating from geographical distributions distinct from those present in the training data.\n",
    "Submissions that achieved success in the competition enhanced conventional neural network modeling and learning methods through the incorporation of ensembling, data augmentation, and transfer learning. Despite these advancements, the limitations became apparent in the winning submissions' ability to predict 'Bride' for images depicting brides from geographies and cultural contexts that lacked sufficient representation during training. This underscores the ongoing need for the refinement of techniques to train classifiers that can generalize effectively across diverse geographical and cultural settings.\n",
    "\n",
    "- Given your exploration in the sections above, what problems might you foresee with [these tasks attempted with the Jigsaw dataset on toxicity](https://link.springer.com/chapter/10.1007/978-981-33-4367-2_81)?\n",
    "\n",
    "\n",
    "In specific instances, comments may require a considerable length (excluding stopwords) to be accurately classified. There is a possibility of misclassification for certain threats and inappropriate comments that are formulated using seemingly \"non-toxic\" words and phrases. The dataset and model excel in the identification and prediction of toxic, abusive, inappropriate comments, and hate speech, particularly prevalent in online comment sections.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca51345e",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Considering questions about ethics which may rise in applications. \n",
    "  \n",
    " \n",
    " Law\n",
    " - How do we balance the use of AI in law enforcement with individuals' right to privacy?\n",
    " - What safeguards should be in place to prevent misuse of AI in surveillance and monitoring?\n",
    " \n",
    " Finance\n",
    " - How can AI applications in finance contribute to financial inclusion without reinforcing existing biases and excluding certain demographic groups?\n",
    " \n",
    " Archives\n",
    " - When AI is used in digitizing and categorizing archival materials, how can we ensure cultural sensitivity and avoid erasing or misrepresenting historical narratives?\n",
    " \n",
    " Generative AI:\n",
    "- Ethics of Content Creation: What ethical considerations arise in the use of generative AI for creating content, especially when it comes to deepfakes, synthetic media, or potentially misleading information?\n",
    "\n",
    "- Ownership and Attribution: Who owns the output generated by AI, and how can we ensure proper attribution and accountability for content created by AI systems?\n",
    "\n",
    "- Malicious Use: How can we mitigate the risk of generative AI being used for malicious purposes, such as creating fake identities, spreading misinformation, or generating harmful content?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cbcb70",
   "metadata": {},
   "source": [
    "# Works Cited\n",
    "    \n",
    "Artificial Intelligence: Development, risks and regulation (2023) lordslibrary.parliament.uk. Available at: https://lordslibrary.parliament.uk/artificial-intelligence-development-risks-and-regulation/ (Accessed: 12 December 2023).\n",
    "\n",
    "Atwood, J. et al. (2019) ‘The Inclusive Images Competition’, The NeurIPS ’18 Competition, pp. 155–186. doi:10.1007/978-3-030-29135-8_6. \n",
    "\n",
    "Inclusive Images Challenge (no date) Kaggle. Available at: https://www.kaggle.com/c/inclusive-images-challenge (Accessed: 12 December 2023). \n",
    "\n",
    "Mahajan, A., Shah, D. and Jafar, G. (2021) ‘Explainable AI approach towards toxic comment classification’, Advances in Intelligent Systems and Computing, pp. 849–858. doi:10.1007/978-981-33-4367-2_81. \n",
    "\n",
    "Office of the Privacy Commissioner of Canada (2023) Privacy tech-know blog: When worlds collide – the possibilities and limits of algorithmic fairness (part 1), Office of the Privacy Commissioner of Canada. Available at: https://www.priv.gc.ca/en/blog/20230405_01/ (Accessed: 12 December 2023). \n",
    "\n",
    "Peller),  dataista0 (Juli&amp;aacute;n (2021) Jigsaw-toxic-comment-classification-challenge, Kaggle. Available at: https://www.kaggle.com/datasets/julian3833/jigsaw-toxic-comment-classification-challenge (Accessed: 12 December 2023). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
